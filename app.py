import streamlit as st
import pandas as pd
import re
from fetch_reddit import fetch_reddit_posts
from fetch_google_trends import fetch_google_trends
from fetch_websearch import fetch_websearch
from merge_and_filter import merge_and_filter
from analyze_gpt import analyze_problem  # ×›×•×œ×œ ×ª××™×›×” ×‘×ª×¨×’×•× ×•×× ×œ×•×’×™×•×ª
from fetch_google_link import search_aliexpress

# --- ×¤×•× ×§×¦×™×” ×œ×‘×“×•×§ ×˜×¨× ×“×™×•×ª ---
def is_trending_topic(df, min_posts=10, min_score=50):
    """
    ×‘×•×“×§×ª ×× ×”× ×•×©× ×—× ××¡×¤×™×§:
    - min_posts: ××™× ×™××•× ×¤×•×¡×˜×™× ×‘×©×‘×•×¢ ×”××—×¨×•×Ÿ
    - min_score: ××™× ×™××•× ×××•×¦×¢ ××™× ×˜×¨××§×¦×™×•×ª (upvotes/comments/views)
    """
    if df.empty:
        return False
    if len(df) < min_posts:
        return False

    # ×‘×“×™×§×ª ××™× ×˜×¨××§×¦×™×”
    if 'score' in df.columns:
        avg_score = df['score'].mean()
    elif 'upvotes' in df.columns:
        avg_score = df['upvotes'].mean()
    elif 'views' in df.columns:
        avg_score = df['views'].mean()
    else:
        avg_score = 0

    return avg_score >= min_score

# --- ×›×•×ª×¨×ª ×”××¤×œ×™×§×¦×™×” ---
st.title("Multi-Source Problem Finder â†’ Product Ideas")

# --- ×§×œ×˜ ××”××©×ª××© ---
keyword = st.text_input("××” ×”×‘×¢×™×” ××• ×”×ª×—×•× ×©×ª×¨×¦×” ×œ×—×¤×©?", "")
min_posts = st.slider("××™× ×™××•× ×¤×•×¡×˜×™× ×‘×©×‘×•×¢ ×”××—×¨×•×Ÿ", 5, 50, 10)
min_score = st.slider("××™× ×™××•× ××™× ×˜×¨××§×¦×™×•×ª ×××•×¦×¢×•×ª ×œ×¤×•×¡×˜", 10, 200, 50)

# --- ×›×¤×ª×•×¨ ×”×¤×¢×œ×” ---
if st.button("Collect & Analyze"):
    if keyword.strip() == "":
        st.warning("×× × ×”×–×Ÿ ××™×œ×” ××• ×ª×—×•× ×œ×—×™×¤×•×©.")
    else:
        st.write(f"××—×¤×© ×‘×¢×™×•×ª ×¢× ××™×œ×ª ××¤×ª×—: **{keyword}** ...")

        # --- ×©×œ×™×¤×ª × ×ª×•× ×™× ×××§×•×¨×•×ª ×©×•× ×™× (×©×‘×•×¢ ××—×¨×•×Ÿ) ---
        reddit_df = fetch_reddit_posts(["BuyItForLife", "LifeProTips"], days=7)
        trends_df = fetch_google_trends()
        tiktok_df = fetch_websearch(keyword, site="tiktok.com")
        quora_df = fetch_websearch(keyword, site="quora.com")

        st.write(f"Reddit results: {len(reddit_df)}, Trends: {len(trends_df)}, TikTok: {len(tiktok_df)}, Quora: {len(quora_df)}")

        # --- ××™×–×•×’ ×•×¡×™× ×•×Ÿ ---
        combined = merge_and_filter([reddit_df, trends_df, tiktok_df, quora_df])

        # --- ×‘×“×™×§×ª ×˜×¨× ×“×™×•×ª ---
        if not is_trending_topic(combined, min_posts, min_score):
            st.warning("×”× ×•×©× ×œ× ×‘×•×¢×¨ ×›×¨×’×¢ (×¤×—×•×ª ××“×™ ×¤×•×¡×˜×™× ××• ×˜×¨××¤×™×§ × ××•×š ×‘×©×‘×•×¢ ×”××—×¨×•×Ÿ).")
        else:
            # --- ×—×™×¤×•×© ×‘×¢×™×•×ª ×©×¨×œ×•×•× ×˜×™×•×ª ×œ××™×œ×ª ×”×—×™×¤×•×© ---
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            filtered = combined[combined["text_clean"].apply(lambda x: bool(pattern.search(str(x))))]

            if not filtered.empty:
                top_problem = filtered.iloc[0]["text_clean"]
            else:
                top_problem = keyword  # fallback â€“ ×©×•×œ×—×™× ×™×©×™×¨×•×ª ××ª ××™×œ×ª ×”×—×™×¤×•×© ×œ-GPT

            st.write(f"### Top Problem Selected:\n{top_problem}")

            # --- ×§×¨×™××” ×œ-GPT (×›×•×œ×œ ×ª×¨×’×•× ×•×× ×œ×•×’×™×”) ---
            gpt_result = analyze_problem(top_problem)
            st.write("**GPT raw output:**", gpt_result)

            # --- ×¢×™×‘×•×“ ×ª×•×¦××•×ª GPT ---
            results = []
            goal_line = None

            for line in gpt_result.split("\n"):
                if line.startswith("Goal:"):
                    goal_line = line.replace("Goal:", "").strip()
                if "Product" in line and "|" in line:
                    try:
                        product_name = line.split("|")[0].split(":")[1].strip()
                        score = line.split("|")[1].split(":")[1].replace("%", "").strip()
                    except Exception:
                        continue

                    # ×§×™×©×•×¨ ×—×™×¤×•×© ××“×•×™×§ ×™×•×ª×¨ (×©× ×”××•×¦×¨ ×‘××œ×•××•)
                    query = product_name.replace(" ", "+")
                    link = f"https://www.aliexpress.com/wholesale?SearchText={query}"

                    results.append({
                        "Product": f"**{product_name}**",
                        "Match": f"{score}%",
                        "Link": f"[ğŸ”— View Product]({link})"
                    })

            # --- ×”×¦×’×ª ×¤×œ×˜ ---
            if results:
                if goal_line:
                    st.write(f"### Goal identified by AI: {goal_line}")
                output_df = pd.DataFrame(results)
                st.write("### Recommended Products")
                st.write(output_df.to_html(escape=False, index=False), unsafe_allow_html=True)
                st.download_button("Download CSV", output_df.to_csv(index=False), "output.csv")
            else:
                st.warning("×œ× ×”×ª×§×‘×œ×• ××•×¦×¨×™× ××ª××™××™×.")
